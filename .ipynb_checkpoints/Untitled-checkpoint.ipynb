{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e96ace90",
   "metadata": {},
   "source": [
    "# Backend Engineering Take-Home Challenge\n",
    "\n",
    "Submitted by: Crystal Nguyen\n",
    "\n",
    "This API allows you to trigger an ETL (Extract, Transform, Load) process and retrieve the results from a PostgreSQL **postgres** database. Uses Flask as the web framework for building the API.\n",
    "\n",
    "## System Requirements\n",
    "\n",
    "To run this application, you need the following system requirements:\n",
    "The application is containerized using Docker. It requires a Docker environment with Docker Engine installed.\n",
    "\n",
    "- Python 3.10\n",
    "- Docker base image: `python:3.10-slim`\n",
    "- PostgreSQL 15.3\n",
    "\n",
    "## Input Requirements\n",
    "\n",
    "```symbol\n",
    "  Root\n",
    "  ├─ Folder 1\n",
    "  │   ├─ File 1\n",
    "  │   └─ File 2\n",
    "  ├─ Folder 2\n",
    "  │   ├─ File 3\n",
    "  │   └─ Subfolder\n",
    "  │       └─ File 4\n",
    "  └─ File 5\n",
    "```\n",
    "To run this application, you need the following input data files:\n",
    "\n",
    "You will find three CSV files in the `data`  directory:\n",
    "\n",
    "- `users.csv`: Contains user data with the following columns: `user_id`, `name`, `email`,`signup_date`.\n",
    "\n",
    "- `user_experiments.csv`: Contains experiment data with the following columns: `experiment_id`, `user_id`, `experiment_compound_ids`, `experiment_run_time`. The `experiment_compound_ids` column contains a semicolon-separated list of compound IDs.\n",
    "\n",
    "\n",
    "- `compounds.csv`: Contains compound data with the following columns: `compound_id`, `compound_name`, `compound_structure`.\n",
    "\n",
    "## Building and Running the Application\n",
    "\n",
    "1. Clone the repository: `git clone <repository_url>`\n",
    "2. Change into the project directory: `cd <project_directory>`\n",
    "3. Build the Docker image: `docker build -t myapp .`\n",
    "4. Run the Docker container: `docker run -d -p 5000:5000 --name myapp_container myapp`\n",
    "5. Access the application at: `http://localhost:5000`\n",
    "\n",
    "### Step 1: Build and Run the Docker Container\n",
    "\n",
    "1. Make sure you have Docker installed on your machine.\n",
    "2. Open a terminal or command prompt.\n",
    "3. Navigate to the project directory.\n",
    "\n",
    "#### Option 1: Using the provided shell script\n",
    "\n",
    "4. Run the `run_app.sh` shell script to build and run the Docker container:\n",
    "\n",
    "```bash\n",
    "./run_app.sh\n",
    "```\n",
    "\n",
    "#### Option 2: Manually build and run the Docker container\n",
    "\n",
    "4. Build the Docker image by running the following command:\n",
    "```bash\n",
    "docker build -t eikon_app:1.0 .\n",
    "```\n",
    "\n",
    "5. Run the Docker container using the following command:\n",
    "```bash\n",
    "docker run -d -p 5000:5000 --name eikon_app_1.0_container eikon_app:1.0\n",
    "```\n",
    "   \n",
    "   \n",
    "`build_and_run.sh`\n",
    "\n",
    "Stop and remove the Docker container\n",
    "\n",
    "```bash\n",
    "docker stop eikon_app_1.0_container`\n",
    "```\n",
    "\n",
    "```bash\n",
    "docker rm eikon_app_1.0_container\n",
    "```\n",
    "\n",
    "### Step 2: Make a Curl Request to the API Endpoint\n",
    "1. Open a new terminal or command prompt.\n",
    "\n",
    "#### Option 1: Using the provided shell script\n",
    "2. Run the make_request.sh shell script to make a curl request to the API endpoint:\n",
    "`run_app.sh`\n",
    "\n",
    "#### Option 2: Manually make a curl request\n",
    "2. Use the following curl command to make a request to the API endpoint:\n",
    "```bash\n",
    "curl -s http://localhost:5000/api-endpoint\n",
    "```\n",
    "\n",
    "Replace **api-endpoint** with the actual endpoint you want to access.\n",
    "\n",
    "## app.py - API Endpoints\n",
    "`app.py`\n",
    "\n",
    "`http://127.0.0.1:5000`\n",
    "\n",
    "### Trigger ETL\n",
    "\n",
    "Endpoint: `/trigger-etl`\n",
    "Method: GET\n",
    "\n",
    "This endpoint triggers the ETL process by calling the `etl()` function. It returns a JSON response indicating the status of the ETL process and any relevant messages.\n",
    "\n",
    "### ETL Results\n",
    "\n",
    "Endpoint: `/etl-results`\n",
    "Method: GET\n",
    "\n",
    "This endpoint retrieves the results of the ETL process. It returns a JSON response containing the ETL results, such as the transformed data or any other relevant output.\n",
    "\n",
    "### Database Structure\n",
    "\n",
    "The Postgres database used by the application consists of the following tables:\n",
    "\n",
    "-  `user_id`:\n",
    "-  `name`:\n",
    "-  `email`:\n",
    "-  `signup_date`:\n",
    "-  `experiment_count`: Total experiments a user ran.\n",
    "-  `avg_experiment_run_time`: Average experiments amount per user.\n",
    "-  `compound_id`: User's most commonly experimented compound.\n",
    "-  `compound_name`:\n",
    "-  `compound_structure`:\n",
    "\n",
    "## Assumptions and Known Issues\n",
    "\n",
    "While the ETL Process API is functional and provides the desired functionality, there are a few known issues and areas that can be improved in future iterations. Here are some of the notable points:\n",
    "\n",
    "- Issue 1: The error handling mechanism can be further improved to provide more detailed and informative error messages to the API consumers.\n",
    "- Improvement 1: Currently, the API only supports GET requests for triggering the ETL process and retrieving results. Adding support for other HTTP methods like POST, PUT, or DELETE can provide more flexibility in controlling the ETL process.\n",
    "- Improvement 2: Enhancing the logging capabilities of the API can help in better monitoring and troubleshooting of the ETL process. Integration with a centralized logging system or implementing log rotation can be considered.\n",
    "- asynchronous processing, based on time\n",
    "- If the ETL process is a background task that needs to be executed periodically or based on certain conditions, it is common to trigger it automatically. In this case, you can set up a scheduled task or use a job scheduler to run the ETL process at predefined intervals or when specific events occur.\n",
    "- On the other hand, if the ETL process is intended to be user-triggered, allowing users to initiate the process at their discretion, you can provide an API endpoint or a user interface that allows users to explicitly trigger the ETL process. This gives users control over when the ETL process is executed.\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "- The ETL process assumes a specific data source format and schema. Ensure that the data provided for the ETL process adheres to the expected structure and format.\n",
    "- The API assumes a stable and accessible database connection with the required credentials and permissions. Ensure that the database is properly set up and accessible before running the API.\n",
    "- assumptions noted in email to aaron\n",
    "\n",
    "\n",
    "### Questions\n",
    "\n",
    "- units\n",
    "- If you have any questions or need clarifications regarding the functionality, usage, or implementation details of the ETL Process API, please don't hesitate to reach out by opening an issue or starting a discussion in the project repository.\n",
    "- We welcome feedback, suggestions, and contributions from the community. If you have ideas for enhancements or improvements, or if you encounter any issues, please let us know.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
