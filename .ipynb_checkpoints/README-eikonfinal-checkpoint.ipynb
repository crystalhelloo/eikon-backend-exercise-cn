{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e96ace90",
   "metadata": {},
   "source": [
    "# Backend Engineering Take-Home Challenge\n",
    "\n",
    "Submitted by: Crystal Nguyen\n",
    "\n",
    "This API allows you to trigger an ETL (Extract, Transform, Load) process and retrieve the results from a **PostgreSQL** database. Uses **Flask** as the web framework for building the API.\n",
    "\n",
    "## System Requirements\n",
    "\n",
    "To run this application, you need the following system requirements:\n",
    "The application is containerized using Docker. It requires a Docker environment with Docker Engine installed.\n",
    "\n",
    "- Python 3.10\n",
    "- Docker base image: `python:3.10-slim`\n",
    "- PostgreSQL 15.3\n",
    "\n",
    "## Input Requirements\n",
    "\n",
    "See below for how the Github repo is structured and the files that are needed to run the app:\n",
    "\n",
    "```symbol\n",
    "  backend_takehome_cn\n",
    "  ├─ data\n",
    "  │   ├─ compounds.csv\n",
    "  │   ├─ user_experiments.csv\n",
    "      └─ users.csv\n",
    "  ├─ templates\n",
    "  │   ├─ error.html\n",
    "  │   └─ success.html\n",
    "  ├─ .dockerignore\n",
    "  ├─ app.py\n",
    "  ├─ build_and_run.sh\n",
    "  ├─ docker-compose.yml\n",
    "  ├─ Dockerfile-app\n",
    "  ├─ Dockerfile-postgres\n",
    "  ├─ fetch_results.sh\n",
    "  ├─ init.sql\n",
    "  ├─ README.md\n",
    "  ├─ requirements.txt\n",
    "  └─ run_app.sh\n",
    "```\n",
    "\n",
    "**Notable files:**\n",
    "- **`README.md`** intro to the app and general usage!\n",
    "- `data` directory contains provided raw input files\n",
    "- `templates` directory contains HTML templates that the Flask app uses to render API request status\n",
    "- **`requirements.txt`** used to install dependences for Docker containers\n",
    "- **`app.py`** file is the entry point for the Flask app\n",
    "- **`docker-compose.yml`** used to handle both PostgreSQL and Flask/Gunicorn containers in single app\n",
    "- **`Dockerfile-app`** for building the Flask/Gunicorn app container, referred to as `eikon-app`\n",
    "- **`Dockerfile-postgres`** for building the PostgreSQL container, referred to as `eikon-db`\n",
    "- `init.sql` used for initializing the PostgreSQL database container\n",
    "- **`build_and_run.sh`** shell script used to build and run Docker containers\n",
    "- **`run_app.sh`** shell script used to make curl requests to API endpoints\n",
    "- **`fetch_results.sh`** shell script used to fetch PostgreSQL table with feature derivation data from the Docker container\n",
    "\n",
    "## Manually build and run the application\n",
    "\n",
    "1. Make sure you have Docker installed on your machine.\n",
    "2. Clone the repository: `git clone <repository_url>`\n",
    "3. Open a terminal or command prompt.\n",
    "4. Change into the project directory: `cd <project_directory>`\n",
    "5. Build and run the Docker container:\n",
    " - **Shell**: Run the `build_and_run.sh` shell script to build and run the Docker container: `./build_and_run.sh`\n",
    " - **Manually**: `docker-compose up -d --build`\n",
    "6. Access the application\n",
    " - **Shell**: Run the `run_app.sh` shell script to make curl requests to the API endpoint: `./run_app.sh`\n",
    " - **Manually**: `curl -s http://localhost:5000/api-endpoint` Replace **api-endpoint** with the actual endpoint you want to access.\n",
    " \n",
    "\n",
    "You can stop and remove existing containers to prevent port conflicts `docker-compose down`\n",
    "\n",
    "## API Endpoints\n",
    "\n",
    "You can also access the app by going to http://127.0.0.1:5000/ in the web browser\n",
    "\n",
    "### Index\n",
    "**Endpoint** `/`\n",
    "\n",
    "HTML response that provides links to available API endpoints, `/trigger-etl` and `/etl-results`\n",
    "\n",
    "### Trigger ETL\n",
    "\n",
    "**Endpoint** `/trigger-etl`\n",
    "\n",
    "**Method** GET\n",
    "\n",
    "Triggers the ETL process by calling the `etl()` function. Returns HTML response with the status of the ETL process.\n",
    "\n",
    "```\n",
    "ETL Process Successful\n",
    "Status Code: 200\n",
    "\n",
    "Message: ETL process completed for 20 rows\n",
    "```\n",
    "\n",
    "### ETL Results\n",
    "\n",
    "**Endpoint** `/etl-results`\n",
    "\n",
    "**Method** GET\n",
    "\n",
    "Retrieves the results of the ETL process. Returns JSON response containing the ETL results, e.g. features table from PostgreSQL container.\n",
    "\n",
    "```\n",
    "[{\"user_id\":1,\"name\":\"Alice\",\"email\":\"alice@example.com\",\"signup_date\":1672531200000,\"experiment_count\":2,\"avg_experiment_run_time\":12.5,\"compound_id\":1,\"compound_name\":\"Compound A\",\"compound_structure\":\"C20H25N3O\"},. . . ]\n",
    "```\n",
    "\n",
    "### Database Structure for PostgreSQL Container\n",
    "\n",
    "**database** `eikon_db`\n",
    "\n",
    "**schema** `sandbox`\n",
    "\n",
    "**table** `features`\n",
    "\n",
    "-  `user_id (bigint)`: ID specifying user, e.g. *1-10*\n",
    "-  `name (text)`: name of the user, e.g. *Alice*\n",
    "-  `email (text)`: user's email address, e.g. *alice@example.com*\n",
    "-  `signup_date (timestamp without time zone)`: user's signup date, e.g. *2023-01-01*\n",
    "-  **`experiment_count (bigint)`**: Total experiments a user ran\n",
    "-  **`avg_experiment_run_time (double precision)`**: Average experiments amount per user.\n",
    "-  `compound_id (bigint)`: User's most commonly experimented compound by compound ID, e.g. *1-3*\n",
    "-  `compound_name (text)`: User's most commonly experimented compound by compound name, e.g. *A-C*\n",
    "-  **`compound_structure (text)`**: User's most commonly experimented compound by compound structure, e.g. *C20H25N3O*\n",
    "\n",
    "## Potential Improvements\n",
    "\n",
    "- Store sensitive database information more securely, e.g. host, username, password, etc.\n",
    "- Gather information from stakeholders on typical raw data files, such as potential areas of standardization and current issues.\n",
    "  - Ask if there's units associated with the numeric measurement columns.\n",
    "  - Incorporate timezone (e.g. UTC) into PostgreSQL table columns with timestamp data type\n",
    "- Assumed how the database, schema, and table are implemented -- see `Database Structure` section. Design PostgreSQL table based on stakeholder use cases.\n",
    "- Application is only validated with the provided input files in the `data` folder. Include testing with more input files to expose edge cases to improve error handling and logging for easier troubleshooting. Set up text fixtures and unit testing.\n",
    "- Potentially add other API requests such as `PUT`,`PATCH`,`DELETE` to add more flexibility to the ETL process and if the scope of the API is expanded.\n",
    "- This API is user-triggered which might be preferred, but assess whether the ETL process should be triggered automatically, such as by using airflow to manage and schedule ETL workflows.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
